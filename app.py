from flask import Flask, render_template, request, jsonify, session
import feedparser
import os
import re
from datetime import datetime, timedelta
import calendar
from urllib.parse import urljoin, urlparse, quote
import html
import chardet
import pytz
import json
import requests
import random
import hashlib
import uuid
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

# Enhanced libraries for better content extraction
try:
    import trafilatura
    TRAFILATURA_AVAILABLE = True
except ImportError:
    TRAFILATURA_AVAILABLE = False

try:
    import newspaper
    from newspaper import Article
    NEWSPAPER_AVAILABLE = True
except ImportError:
    NEWSPAPER_AVAILABLE = False

try:
    from bs4 import BeautifulSoup
    BEAUTIFULSOUP_AVAILABLE = True
except ImportError:
    BEAUTIFULSOUP_AVAILABLE = False

# Gemini AI for content analysis
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

# Flask app configuration
app = Flask(__name__)
app.secret_key = os.getenv('SECRET_KEY', 'tien-phong-econ-portal-2025')

# Environment variables
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')

# Timezone - Vietnam
VN_TIMEZONE = pytz.timezone('Asia/Ho_Chi_Minh')
UTC_TIMEZONE = pytz.UTC

# User cache with enhanced management
user_news_cache = {}
user_last_detail_cache = {}
global_seen_articles = {}
MAX_CACHE_ENTRIES = 15  # Reduced for memory optimization
MAX_GLOBAL_CACHE = 300  # Reduced significantly
CACHE_EXPIRE_HOURS = 4  # Reduced cache time

# Enhanced User Agents for better compatibility
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]

# Configure logging for better debugging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_current_vietnam_datetime():
    """Get current Vietnam date and time"""
    return datetime.now(VN_TIMEZONE)

def get_current_date_str():
    """Get current date string in Vietnam format"""
    current_dt = get_current_vietnam_datetime()
    return current_dt.strftime("%d/%m/%Y")

def get_current_time_str():
    """Get current time string in Vietnam format"""
    current_dt = get_current_vietnam_datetime()
    return current_dt.strftime("%H:%M")

def get_current_datetime_str():
    """Get current datetime string for display"""
    current_dt = get_current_vietnam_datetime()
    return current_dt.strftime("%H:%M %d/%m/%Y")

print("ğŸš€ Tiá»n Phong E-con News Backend - FIXED SYNC VERSION:")
print(f"Gemini AI: {'âœ…' if GEMINI_API_KEY else 'âŒ'}")
print(f"Content Extraction: {'âœ…' if TRAFILATURA_AVAILABLE else 'âŒ'}")
print("=" * 50)

# OPTIMIZED RSS FEEDS - Reduced for better performance
RSS_FEEDS = {
    # === CAFEF RSS FEEDS (Primary Vietnamese Source) ===
    'cafef': {
        'cafef_stocks': 'https://cafef.vn/thi-truong-chung-khoan.rss',
        'cafef_business': 'https://cafef.vn/doanh-nghiep.rss',
        'cafef_finance': 'https://cafef.vn/tai-chinh-ngan-hang.rss'
    },
    
    # === INTERNATIONAL RSS FEEDS (Limited for performance) ===
    'international': {
        'yahoo_finance': 'https://finance.yahoo.com/news/rssindex',
        'marketwatch': 'https://feeds.content.dowjones.io/public/rss/mw_topstories',
        'cnbc': 'https://www.cnbc.com/id/100003114/device/rss/rss.html'
    }
}

# OPTIMIZED CATEGORY MAPPING
CATEGORY_MAPPING = {
    'all': ['cafef', 'international'],
    'domestic': ['cafef'],
    'international': ['international'],
    'stocks': ['cafef_stocks'],
    'business': ['cafef_business'],
    'finance': ['cafef_finance', 'yahoo_finance'],
    'realestate': ['cafef_business'],
    'crypto': ['yahoo_finance'],
    'earnings': ['cafef_business'],
    'projects': ['cafef_business'],
    'macro': ['cafef_finance'],
    'world': ['international'],
    'hot': ['cafef_stocks', 'yahoo_finance'],
    'tech': ['cnbc']
}

def convert_utc_to_vietnam_time(utc_time_tuple):
    """Convert UTC to Vietnam time"""
    try:
        utc_timestamp = calendar.timegm(utc_time_tuple)
        utc_dt = datetime.fromtimestamp(utc_timestamp, tz=UTC_TIMEZONE)
        vn_dt = utc_dt.astimezone(VN_TIMEZONE)
        return vn_dt
    except Exception as e:
        return datetime.now(VN_TIMEZONE)

def normalize_title(title):
    """Normalize title for exact comparison"""
    normalized = re.sub(r'\s+', ' ', title.lower().strip())
    normalized = re.sub(r'[.,!?;:\-\u2013\u2014]', '', normalized)
    normalized = re.sub(r'["\'\u201c\u201d\u2018\u2019]', '', normalized)
    return normalized

def clean_expired_cache():
    """Clean expired articles from global cache"""
    global global_seen_articles
    current_time = get_current_vietnam_datetime()
    expired_hashes = []
    
    for article_hash, article_data in global_seen_articles.items():
        time_diff = current_time - article_data['timestamp']
        if time_diff.total_seconds() > (CACHE_EXPIRE_HOURS * 3600):
            expired_hashes.append(article_hash)
    
    for expired_hash in expired_hashes:
        del global_seen_articles[expired_hash]
    
    if expired_hashes:
        logger.info(f"ğŸ§¹ Cleaned {len(expired_hashes)} expired articles from cache")

def is_duplicate_article_local(news_item, existing_articles):
    """Check duplicate within current collection"""
    current_title = normalize_title(news_item['title'])
    current_link = news_item['link'].lower().strip()
    
    for existing in existing_articles:
        existing_title = normalize_title(existing['title'])
        existing_link = existing['link'].lower().strip()
        
        if current_title == existing_title or current_link == existing_link:
            return True
    
    return False

def is_duplicate_article_global(news_item, source_name):
    """Check duplicate against global cache"""
    global global_seen_articles
    
    try:
        clean_expired_cache()
        
        current_title = normalize_title(news_item['title'])
        current_link = news_item['link'].lower().strip()
        
        for existing_data in global_seen_articles.values():
            existing_title = normalize_title(existing_data['title'])
            existing_link = existing_data['link'].lower().strip()
            
            if current_title == existing_title or current_link == existing_link:
                return True
        
        cache_key = f"{current_title}|{current_link}"
        
        global_seen_articles[cache_key] = {
            'title': news_item['title'],
            'link': news_item['link'],
            'source': source_name,
            'timestamp': get_current_vietnam_datetime()
        }
        
        if len(global_seen_articles) > MAX_GLOBAL_CACHE:
            sorted_items = sorted(global_seen_articles.items(), key=lambda x: x[1]['timestamp'])
            for old_key, _ in sorted_items[:50]:
                del global_seen_articles[old_key]
        
        return False
        
    except Exception as e:
        logger.warning(f"âš ï¸ Global duplicate check error: {e}")
        return False

def get_enhanced_headers(url=None):
    """Enhanced headers for better compatibility"""
    user_agent = random.choice(USER_AGENTS)
    
    headers = {
        'User-Agent': user_agent,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'DNT': '1'
    }
    
    if url:
        if 'cafef.vn' in url.lower():
            headers.update({
                'Referer': 'https://cafef.vn/',
                'Origin': 'https://cafef.vn'
            })
        elif 'yahoo' in url.lower():
            headers.update({
                'Referer': 'https://finance.yahoo.com/',
                'Origin': 'https://finance.yahoo.com'
            })
    
    return headers

def is_international_source(source_name):
    """Check if source is international"""
    international_sources = [
        'yahoo_finance', 'marketwatch', 'cnbc', 'reuters', 'investing_com', 
        'bloomberg', 'financial_times', 'wsj_markets'
    ]
    return any(source in source_name for source in international_sources)

def fetch_rss_with_requests(rss_url, timeout=10):
    """Fetch RSS using requests with timeout"""
    try:
        headers = get_enhanced_headers(rss_url)
        response = requests.get(rss_url, headers=headers, timeout=timeout)
        
        if response.status_code == 200:
            return response.content
        else:
            logger.warning(f"âŒ HTTP {response.status_code} for {rss_url}")
            return None
    except Exception as e:
        logger.warning(f"âŒ Fetch error for {rss_url}: {e}")
        return None

def process_rss_feed_sync(source_name, rss_url, limit_per_source):
    """SYNC RSS feed processing - NO MORE ASYNC"""
    try:
        logger.info(f"ğŸ”„ Processing {source_name}: {rss_url}")
        
        # Fetch RSS content with timeout
        content = fetch_rss_with_requests(rss_url, timeout=8)
        
        if content:
            feed = feedparser.parse(content)
        else:
            feed = feedparser.parse(rss_url)
        
        if not feed or not hasattr(feed, 'entries') or len(feed.entries) == 0:
            logger.warning(f"âŒ No entries found for {source_name}")
            return []
        
        news_items = []
        for entry in feed.entries[:limit_per_source]:
            try:
                vn_time = get_current_vietnam_datetime()
                
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    vn_time = convert_utc_to_vietnam_time(entry.published_parsed)
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    vn_time = convert_utc_to_vietnam_time(entry.updated_parsed)
                
                description = ""
                if hasattr(entry, 'summary'):
                    description = entry.summary[:250] + "..." if len(entry.summary) > 250 else entry.summary
                elif hasattr(entry, 'description'):
                    description = entry.description[:250] + "..." if len(entry.description) > 250 else entry.description
                
                if hasattr(entry, 'title') and hasattr(entry, 'link'):
                    title = entry.title.strip()
                    
                    # Enhanced relevance filtering
                    if is_relevant_news(title, description, source_name):
                        news_item = {
                            'title': html.unescape(title),
                            'link': entry.link,
                            'source': source_name,
                            'published': vn_time,
                            'published_str': vn_time.strftime("%H:%M %d/%m"),
                            'description': html.unescape(description) if description else ""
                        }
                        news_items.append(news_item)
                
            except Exception as entry_error:
                logger.warning(f"âš ï¸ Entry processing error: {entry_error}")
                continue
        
        logger.info(f"âœ… Processed {len(news_items)} articles from {source_name}")
        return news_items
        
    except Exception as e:
        logger.error(f"âŒ RSS processing error for {source_name}: {e}")
        return []

def is_relevant_news(title, description, source_name):
    """Enhanced relevance filtering"""
    # CafeF sources are always relevant
    if 'cafef' in source_name:
        return True
    
    # For international sources, use enhanced keyword filtering
    financial_keywords = [
        # English keywords
        'stock', 'market', 'trading', 'investment', 'economy', 'economic',
        'bitcoin', 'crypto', 'currency', 'bank', 'financial', 'finance',
        'earnings', 'revenue', 'profit', 'inflation', 'fed', 'gdp',
        'business', 'company', 'corporate', 'industry', 'sector',
        'money', 'cash', 'capital', 'fund', 'price', 'cost', 'value',
        'growth', 'analyst', 'forecast', 'report', 'data', 'sales',
        # Vietnamese keywords
        'chá»©ng khoÃ¡n', 'tÃ i chÃ­nh', 'ngÃ¢n hÃ ng', 'kinh táº¿', 'Ä‘áº§u tÆ°',
        'doanh nghiá»‡p', 'thá»‹ trÆ°á»ng', 'cá»• phiáº¿u', 'lá»£i nhuáº­n'
    ]
    
    title_lower = title.lower()
    description_lower = description.lower() if description else ""
    combined_text = f"{title_lower} {description_lower}"
    
    # Check for keywords
    keyword_count = sum(1 for keyword in financial_keywords if keyword in combined_text)
    
    # More relaxed filtering - accept if at least one keyword or if it's business-related
    return keyword_count > 0 or any(word in combined_text for word in ['business', 'company', 'market', 'economic'])

def process_single_source_sync(source_name, source_url, limit_per_source):
    """Process a single RSS source synchronously"""
    try:
        if source_url.endswith('.rss') or 'rss' in source_url.lower() or 'feeds.' in source_url:
            return process_rss_feed_sync(source_name, source_url, limit_per_source)
        else:
            logger.warning(f"âš ï¸ Unsupported URL format for {source_name}")
            return []
            
    except Exception as e:
        logger.error(f"âŒ Error processing {source_name}: {e}")
        return []

def collect_news_enhanced_sync(sources_dict, limit_per_source=8, use_global_dedup=True):
    """SYNC news collection with threading for better performance"""
    all_news = []
    
    logger.info(f"ğŸ”„ Starting SYNC collection from {len(sources_dict)} sources")
    
    if use_global_dedup:
        clean_expired_cache()
    
    # Use ThreadPoolExecutor for concurrent processing without async
    with ThreadPoolExecutor(max_workers=3) as executor:  # Limited workers for Render.com
        # Submit all tasks
        future_to_source = {
            executor.submit(process_single_source_sync, source_name, source_url, limit_per_source): source_name
            for source_name, source_url in sources_dict.items()
        }
        
        # Collect results with timeout
        total_processed = 0
        local_duplicates = 0
        global_duplicates = 0
        
        for future in as_completed(future_to_source, timeout=15):  # 15 second timeout
            source_name = future_to_source[future]
            try:
                result = future.result(timeout=5)  # 5 second per source timeout
                
                if result:
                    for news_item in result:
                        total_processed += 1
                        
                        # Local duplicate check
                        if is_duplicate_article_local(news_item, all_news):
                            local_duplicates += 1
                            continue
                        
                        # Global duplicate check
                        if use_global_dedup and is_duplicate_article_global(news_item, news_item['source']):
                            global_duplicates += 1
                            continue
                        
                        # Add unique article
                        all_news.append(news_item)
                        
            except Exception as e:
                logger.error(f"âŒ Source {source_name} processing error: {e}")
                continue
    
    unique_count = len(all_news)
    logger.info(f"ğŸ“Š Collection results: {total_processed} processed, {local_duplicates} local dups, {global_duplicates} global dups, {unique_count} unique")
    
    # Sort by publish time (newest first)
    all_news.sort(key=lambda x: x['published'], reverse=True)
    return all_news

def get_or_create_user_session():
    """Get or create user session ID"""
    if 'user_id' not in session:
        session['user_id'] = str(uuid.uuid4())
    return session['user_id']

def save_user_news_enhanced(user_id, news_list, command_type, current_page=1):
    """Enhanced user news saving with pagination info"""
    global user_news_cache
    
    user_news_cache[user_id] = {
        'news': news_list,
        'command': command_type,
        'current_page': current_page,
        'timestamp': get_current_vietnam_datetime()
    }
    
    # Clean up old cache entries
    if len(user_news_cache) > MAX_CACHE_ENTRIES:
        oldest_users = sorted(user_news_cache.items(), key=lambda x: x[1]['timestamp'])[:5]
        for user_id_to_remove, _ in oldest_users:
            del user_news_cache[user_id_to_remove]

def save_user_last_detail(user_id, news_item):
    """Save last article accessed for AI context"""
    global user_last_detail_cache
    
    user_last_detail_cache[user_id] = {
        'article': news_item,
        'timestamp': get_current_vietnam_datetime()
    }

# SYNC AI Engine - No more async
class GeminiAIEngine:
    def __init__(self):
        self.available = GEMINI_AVAILABLE and GEMINI_API_KEY
        if self.available:
            genai.configure(api_key=GEMINI_API_KEY)
    
    def ask_question(self, question: str, context: str = ""):
        """SYNC Gemini AI question answering"""
        if not self.available:
            return "âš ï¸ Gemini AI khÃ´ng kháº£ dá»¥ng. Vui lÃ²ng kiá»ƒm tra cáº¥u hÃ¬nh API."
        
        try:
            current_date_str = get_current_date_str()
            
            prompt = f"""Báº¡n lÃ  Gemini AI - chuyÃªn gia tÃ i chÃ­nh chá»©ng khoÃ¡n thÃ´ng minh. HÃ£y tráº£ lá»i cÃ¢u há»i vá»›i kiáº¿n thá»©c chuyÃªn sÃ¢u.

CÃ‚U Há»I: {question}

{f"Bá»I Cáº¢NH: {context}" if context else ""}

HÆ¯á»šNG DáºªN TRáº¢ Lá»œI:
1. Sá»­ dá»¥ng kiáº¿n thá»©c tÃ i chÃ­nh chuyÃªn mÃ´n sÃ¢u rá»™ng
2. ÄÆ°a ra phÃ¢n tÃ­ch chi tiáº¿t vÃ  toÃ n diá»‡n
3. Káº¿t ná»‘i vá»›i bá»‘i cáº£nh thá»‹ trÆ°á»ng hiá»‡n táº¡i (ngÃ y {current_date_str})
4. ÄÆ°a ra vÃ­ dá»¥ thá»±c táº¿ tá»« thá»‹ trÆ°á»ng Viá»‡t Nam vÃ  quá»‘c táº¿
5. Äá»™ dÃ i: 300-500 tá»« vá»›i cáº¥u trÃºc rÃµ rÃ ng
6. Sá»­ dá»¥ng **TiÃªu Ä‘á»** Ä‘á»ƒ tá»• chá»©c ná»™i dung
7. TÃ¡ch dÃ²ng rÃµ rÃ ng giá»¯a cÃ¡c Ä‘oáº¡n vÄƒn
8. ÄÆ°a ra káº¿t luáº­n vÃ  khuyáº¿n nghá»‹ cá»¥ thá»ƒ

HÃ£y thá»ƒ hiá»‡n chuyÃªn mÃ´n vÃ  kiáº¿n thá»©c sÃ¢u rá»™ng cá»§a Gemini AI:"""

            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            generation_config = genai.types.GenerationConfig(
                temperature=0.2,
                top_p=0.8,
                max_output_tokens=1200,
            )
            
            response = model.generate_content(prompt, generation_config=generation_config)
            return response.text.strip()
            
        except Exception as e:
            return f"âš ï¸ Lá»—i Gemini AI: {str(e)}"
    
    def debate_perspectives(self, topic: str):
        """SYNC multi-perspective debate system"""
        if not self.available:
            return "âš ï¸ Gemini AI khÃ´ng kháº£ dá»¥ng cho chá»©c nÄƒng bÃ n luáº­n."
        
        try:
            prompt = f"""Tá»• chá»©c cuá»™c tranh luáº­n chuyÃªn sÃ¢u vá»: {topic}

YÃŠU Cáº¦U Äáº¶C BIá»†T: Má»—i nhÃ¢n váº­t pháº£i cÃ³ pháº§n riÃªng biá»‡t, dá»… tÃ¡ch thÃ nh cÃ¡c tin nháº¯n riÃªng láº».

Há»† THá»NG 6 QUAN ÄIá»‚M (NHÃ‚N Váº¬T Gá»C):

ğŸ“ **GS Äáº¡i há»c (Há»c thuáº­t chÃ­nh trá»±c):**
[Phong cÃ¡ch: Há»c thuáº­t nghiÃªm tÃºc, dá»±a trÃªn nghiÃªn cá»©u vÃ  lÃ½ thuyáº¿t kinh táº¿]
[TrÃ¬nh bÃ y quan Ä‘iá»ƒm 80-120 tá»«, káº¿t thÃºc vá»›i dáº¥u cháº¥m cÃ¢u.]

ğŸ’° **NhÃ  kinh táº¿ há»c (Tham nhÅ©ng tinh vi):**
[Phong cÃ¡ch: PhÃ¢n tÃ­ch sÃ¢u nhÆ°ng cÃ³ xu hÆ°á»›ng á»§ng há»™ lá»£i Ã­ch nhÃ³m]
[TrÃ¬nh bÃ y quan Ä‘iá»ƒm 80-120 tá»«, káº¿t thÃºc vá»›i dáº¥u cháº¥m cÃ¢u.]

ğŸ¢ **NhÃ¢n viÃªn cÃ´ng sá»Ÿ (Ham tiá»n thá»±c táº¿):**
[Phong cÃ¡ch: Thá»±c táº¿, táº­p trung vÃ o lá»£i Ã­ch cÃ¡ nhÃ¢n vÃ  thu nháº­p]
[TrÃ¬nh bÃ y quan Ä‘iá»ƒm 80-120 tá»«, káº¿t thÃºc vá»›i dáº¥u cháº¥m cÃ¢u.]

ğŸ˜Ÿ **NgÆ°á»i nghÃ¨o (Kiáº¿n thá»©c háº¡n háº¹p):**
[Phong cÃ¡ch: ÄÆ¡n giáº£n, lo láº¯ng vá» cuá»™c sá»‘ng hÃ ng ngÃ y]
[TrÃ¬nh bÃ y quan Ä‘iá»ƒm 80-120 tá»«, káº¿t thÃºc vá»›i dáº¥u cháº¥m cÃ¢u.]

ğŸ’ **Äáº¡i gia (NgÆ°á»i giÃ u Ã­ch ká»·):**
[Phong cÃ¡ch: Tá»± tin, chá»‰ quan tÃ¢m Ä‘áº¿n lá»£i nhuáº­n vÃ  tháº¿ lá»±c]
[TrÃ¬nh bÃ y quan Ä‘iá»ƒm 80-120 tá»«, káº¿t thÃºc vá»›i dáº¥u cháº¥m cÃ¢u.]

ğŸ¦ˆ **Shark (NgÆ°á»i giÃ u thÃ´ng thÃ¡i):**
[Phong cÃ¡ch: Kinh nghiá»‡m thá»±c táº¿, táº§m nhÃ¬n dÃ i háº¡n, cÃ¢n báº±ng]
[TrÃ¬nh bÃ y quan Ä‘iá»ƒm 80-120 tá»«, káº¿t thÃºc vá»›i dáº¥u cháº¥m cÃ¢u.]

QUAN TRá»ŒNG: Má»—i nhÃ¢n váº­t pháº£i cÃ³ pháº§n riÃªng biá»‡t, báº¯t Ä‘áº§u vá»›i emoji vÃ  tÃªn, káº¿t thÃºc rÃµ rÃ ng."""

            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            generation_config = genai.types.GenerationConfig(
                temperature=0.4,
                top_p=0.9,
                max_output_tokens=1500,
            )
            
            response = model.generate_content(prompt, generation_config=generation_config)
            return response.text.strip()
            
        except Exception as e:
            return f"âš ï¸ Lá»—i Gemini AI: {str(e)}"
    
    def analyze_article(self, article_content: str, question: str = ""):
        """SYNC article analysis"""
        if not self.available:
            return "âš ï¸ Gemini AI khÃ´ng kháº£ dá»¥ng cho phÃ¢n tÃ­ch bÃ i bÃ¡o."
        
        try:
            analysis_question = question if question else "HÃ£y phÃ¢n tÃ­ch vÃ  tÃ³m táº¯t bÃ i bÃ¡o nÃ y"
            
            # Optimize content length
            if len(article_content) > 3000:
                article_content = article_content[:3000] + "..."
            
            prompt = f"""Báº¡n lÃ  Gemini AI - chuyÃªn gia phÃ¢n tÃ­ch tÃ i chÃ­nh hÃ ng Ä‘áº§u. HÃ£y phÃ¢n tÃ­ch bÃ i bÃ¡o dá»±a trÃªn Ná»˜I DUNG ÄÆ¯á»¢C CUNG Cáº¤P.

**Ná»˜I DUNG BÃ€I BÃO:**
{article_content}

**YÃŠU Cáº¦U PHÃ‚N TÃCH:**
{analysis_question}

**HÆ¯á»šNG DáºªN PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:**
1. PhÃ¢n tÃ­ch CHá»¦ Yáº¾U dá»±a trÃªn ná»™i dung bÃ i bÃ¡o (90%)
2. Bá»• sung kiáº¿n thá»©c chuyÃªn mÃ´n Ä‘á»ƒ giáº£i thÃ­ch sÃ¢u hÆ¡n (10%)
3. Sá»­ dá»¥ng **TiÃªu Ä‘á»** Ä‘á»ƒ tá»• chá»©c ná»™i dung
4. TÃ¡ch dÃ²ng rÃµ rÃ ng giá»¯a cÃ¡c Ä‘oáº¡n vÄƒn
5. PhÃ¢n tÃ­ch tÃ¡c Ä‘á»™ng, nguyÃªn nhÃ¢n, háº­u quáº£ chi tiáº¿t
6. ÄÆ°a ra nháº­n Ä‘á»‹nh vÃ  Ä‘Ã¡nh giÃ¡ chuyÃªn mÃ´n
7. Tráº£ lá»i cÃ¢u há»i trá»±c tiáº¿p vá»›i báº±ng chá»©ng tá»« bÃ i bÃ¡o
8. Äá»™ dÃ i: 400-700 tá»« vá»›i cáº¥u trÃºc rÃµ rÃ ng
9. Tham chiáº¿u cÃ¡c pháº§n cá»¥ thá»ƒ trong bÃ i bÃ¡o
10. ÄÆ°a ra káº¿t luáº­n vÃ  khuyáº¿n nghá»‹

**QUAN TRá»ŒNG:** Táº­p trung hoÃ n toÃ n vÃ o ná»™i dung bÃ i bÃ¡o. ÄÆ°a ra phÃ¢n tÃ­ch CHUYÃŠN SÃ‚U vÃ  CHI TIáº¾T:"""

            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            generation_config = genai.types.GenerationConfig(
                temperature=0.2,
                top_p=0.8,
                max_output_tokens=1800,
            )
            
            response = model.generate_content(prompt, generation_config=generation_config)
            return response.text.strip()
            
        except Exception as e:
            return f"âš ï¸ Lá»—i Gemini AI: {str(e)}"

# Initialize Gemini Engine
gemini_engine = GeminiAIEngine()

# FIXED source mapping for display
source_names = {
    # CafeF sources  
    'cafef_stocks': 'CafeF CK', 'cafef_business': 'CafeF DN',
    'cafef_realestate': 'CafeF BÄS', 'cafef_finance': 'CafeF TC',
    'cafef_macro': 'CafeF VM',
    
    # International sources
    'yahoo_finance': 'Yahoo Finance', 'marketwatch': 'MarketWatch',
    'cnbc': 'CNBC', 'reuters_business': 'Reuters', 
    'investing_com': 'Investing.com', 'bloomberg': 'Bloomberg',
    'financial_times': 'Financial Times', 'wsj_markets': 'WSJ Markets'
}

emoji_map = {
    # CafeF sources
    'cafef_stocks': 'ğŸ“Š', 'cafef_business': 'ğŸ­', 'cafef_realestate': 'ğŸ˜ï¸',
    'cafef_finance': 'ğŸ’³', 'cafef_macro': 'ğŸ“‰',
    
    # International sources
    'yahoo_finance': 'ğŸ’¼', 'marketwatch': 'ğŸ“°', 'cnbc': 'ğŸ“º',
    'reuters_business': 'ğŸŒ', 'investing_com': 'ğŸ’¹', 'bloomberg': 'ğŸ“Š',
    'financial_times': 'ğŸ“ˆ', 'wsj_markets': 'ğŸ’¹'
}

# Flask Routes - ALL SYNC NOW
@app.route('/')
def index():
    """Main page with traditional newspaper theme"""
    return render_template('index.html')

@app.route('/api/news/<news_type>')
def get_news_api(news_type):
    """SYNC API endpoint for getting news"""
    try:
        page = int(request.args.get('page', 1))
        user_id = get_or_create_user_session()
        
        logger.info(f"ğŸ” API request: /api/news/{news_type}?page={page}")
        
        # Map frontend categories to RSS feeds
        if news_type in CATEGORY_MAPPING:
            # Get RSS feed categories for this news type
            feed_categories = CATEGORY_MAPPING[news_type]
            
            # Collect RSS URLs
            all_sources = {}
            for category in feed_categories:
                if category == 'cafef':
                    all_sources.update(RSS_FEEDS['cafef'])
                elif category == 'international':
                    all_sources.update(RSS_FEEDS['international'])
                elif category in RSS_FEEDS['cafef']:
                    all_sources[category] = RSS_FEEDS['cafef'][category]
                elif category in RSS_FEEDS['international']:
                    all_sources[category] = RSS_FEEDS['international'][category]
            
            logger.info(f"ğŸ“¡ Collecting from {len(all_sources)} sources for {news_type}")
            all_news = collect_news_enhanced_sync(all_sources, 8)  # Reduced limit
            
        else:
            logger.error(f"âŒ Invalid news type: {news_type}")
            return jsonify({'error': f'Invalid news type: {news_type}'}), 400
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = all_news[start_index:end_index]
        
        # Save to user cache
        save_user_news_enhanced(user_id, all_news, f"{news_type}_page_{page}")
        
        # Format news for frontend
        formatted_news = []
        for i, news in enumerate(page_news):
            emoji = emoji_map.get(news['source'], 'ğŸ“°')
            source_display = source_names.get(news['source'], news['source'])
            
            formatted_news.append({
                'id': start_index + i,
                'title': news['title'],
                'link': news['link'],
                'source': source_display,
                'emoji': emoji,
                'published': news['published_str'],
                'description': news['description'][:200] + "..." if len(news['description']) > 200 else news['description']
            })
        
        total_pages = (len(all_news) + items_per_page - 1) // items_per_page
        
        logger.info(f"âœ… API success: {len(formatted_news)} articles, page {page}/{total_pages}")
        
        return jsonify({
            'news': formatted_news,
            'page': page,
            'total_pages': total_pages,
            'total_articles': len(all_news)
        })
        
    except Exception as e:
        logger.error(f"âŒ API error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/article/<int:article_id>')
def get_article_detail(article_id):
    """SYNC article detail endpoint"""
    try:
        user_id = get_or_create_user_session()
        
        if user_id not in user_news_cache:
            return jsonify({
                'error': 'PhiÃªn lÃ m viá»‡c Ä‘Ã£ háº¿t háº¡n. Vui lÃ²ng lÃ m má»›i trang.',
                'error_code': 'SESSION_EXPIRED'
            }), 404
            
        user_data = user_news_cache[user_id]
        news_list = user_data['news']
        
        if not news_list or article_id < 0 or article_id >= len(news_list):
            return jsonify({
                'error': f'ID bÃ i viáº¿t khÃ´ng há»£p lá»‡. Pháº¡m vi: 0-{len(news_list)-1}.',
                'error_code': 'INVALID_ARTICLE_ID'
            }), 404
            
        news = news_list[article_id]
        
        # Save as last detail for AI context
        save_user_last_detail(user_id, news)
        
        source_display = source_names.get(news['source'], news['source'])
        
        # IFRAME MODE: Return article link for iframe display
        return jsonify({
            'title': news['title'],
            'link': news['link'],  # Frontend will use this for iframe
            'source': source_display,
            'published': news['published_str'],
            'iframe_mode': True,  # Flag to indicate iframe mode
            'success': True
        })
        
    except Exception as e:
        logger.error(f"âŒ Article detail error: {e}")
        return jsonify({
            'error': 'Lá»—i há»‡ thá»‘ng khi táº£i bÃ i viáº¿t.',
            'error_code': 'SYSTEM_ERROR',
            'details': str(e)
        }), 500

@app.route('/api/ai/ask', methods=['POST'])
def ai_ask():
    """SYNC AI ask endpoint"""
    try:
        data = request.get_json()
        question = data.get('question', '')
        user_id = get_or_create_user_session()
        
        # Check for recent article context
        context = ""
        if user_id in user_last_detail_cache:
            last_detail = user_last_detail_cache[user_id]
            time_diff = get_current_vietnam_datetime() - last_detail['timestamp']
            
            if time_diff.total_seconds() < 1800:  # 30 minutes
                article = last_detail['article']
                context = f"BÃ€I BÃO HIá»†N Táº I:\nTiÃªu Ä‘á»: {article['title']}\nNguá»“n: {article['source']}\nMÃ´ táº£: {article['description']}"
        
        # Get AI response (SYNC)
        if context and not question:
            # Auto-summarize if no question provided
            response = gemini_engine.ask_question("HÃ£y tÃ³m táº¯t vÃ  phÃ¢n tÃ­ch bÃ i bÃ¡o hiá»‡n táº¡i", context)
        else:
            response = gemini_engine.ask_question(question, context)
        
        return jsonify({'response': response})
        
    except Exception as e:
        logger.error(f"âŒ AI ask error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/ai/debate', methods=['POST'])
def ai_debate():
    """SYNC AI debate endpoint"""
    try:
        data = request.get_json()
        topic = data.get('topic', '')
        user_id = get_or_create_user_session()
        
        # Check for context if no topic provided
        if not topic:
            if user_id in user_last_detail_cache:
                last_detail = user_last_detail_cache[user_id]
                time_diff = get_current_vietnam_datetime() - last_detail['timestamp']
                
                if time_diff.total_seconds() < 1800:
                    article = last_detail['article']
                    topic = f"BÃ i bÃ¡o: {article['title']}"
                else:
                    return jsonify({'error': 'KhÃ´ng cÃ³ chá»§ Ä‘á» Ä‘á»ƒ bÃ n luáº­n'}), 400
            else:
                return jsonify({'error': 'Cáº§n nháº­p chá»§ Ä‘á» Ä‘á»ƒ bÃ n luáº­n'}), 400
        
        response = gemini_engine.debate_perspectives(topic)
        
        return jsonify({'response': response})
        
    except Exception as e:
        logger.error(f"âŒ AI debate error: {e}")
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    # Configure Gemini if available
    if GEMINI_API_KEY and GEMINI_AVAILABLE:
        genai.configure(api_key=GEMINI_API_KEY)
        logger.info("âœ… Gemini AI configured successfully")
    
    logger.info("ğŸš€ Tiá»n Phong E-con News Backend starting (FIXED SYNC VERSION)...")
    logger.info(f"ğŸ“Š Category mappings: {len(CATEGORY_MAPPING)} categories")
    logger.info(f"ğŸ“¡ RSS sources: {sum(len(feeds) for feeds in RSS_FEEDS.values())}")
    logger.info("âœ… Fixed SYNC endpoints - NO MORE ASYNC ISSUES!")
    logger.info("=" * 50)
    
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 8080)), debug=False)
