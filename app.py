from flask import Flask, render_template, request, jsonify, session
import feedparser
import asyncio
import os
import re
from datetime import datetime, timedelta
import calendar
from urllib.parse import urljoin, urlparse, quote
import html
import chardet
import pytz
import json
import aiohttp
from enum import Enum
from typing import List, Dict, Tuple, Optional
import random
import hashlib
import uuid

# üöÄ OPTIMIZED LIBRARIES - Enhanced for async operations
try:
    import trafilatura
    TRAFILATURA_AVAILABLE = True
except ImportError:
    TRAFILATURA_AVAILABLE = False

try:
    import newspaper
    from newspaper import Article
    NEWSPAPER_AVAILABLE = True
except ImportError:
    NEWSPAPER_AVAILABLE = False

try:
    from bs4 import BeautifulSoup
    BEAUTIFULSOUP_AVAILABLE = True
except ImportError:
    BEAUTIFULSOUP_AVAILABLE = False

# üÜï GEMINI ONLY - Enhanced AI System with Direct Content Access
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

# Flask app configuration
app = Flask(__name__)
app.secret_key = os.getenv('SECRET_KEY', 'your-secret-key-change-in-production')

# üîí ENVIRONMENT VARIABLES
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')

# üîß TIMEZONE - Vietnam
VN_TIMEZONE = pytz.timezone('Asia/Ho_Chi_Minh')
UTC_TIMEZONE = pytz.UTC

# User cache with deduplication
user_news_cache = {}
user_last_detail_cache = {}
global_seen_articles = {}  # Global deduplication cache
MAX_CACHE_ENTRIES = 25
MAX_GLOBAL_CACHE = 1000
CACHE_EXPIRE_HOURS = 24

# üîß Enhanced User Agents for better compatibility
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]

def get_current_vietnam_datetime():
    """Get current Vietnam date and time"""
    return datetime.now(VN_TIMEZONE)

def get_current_date_str():
    """Get current date string in Vietnam format"""
    current_dt = get_current_vietnam_datetime()
    return current_dt.strftime("%d/%m/%Y")

def get_current_time_str():
    """Get current time string in Vietnam format"""
    current_dt = get_current_vietnam_datetime()
    return current_dt.strftime("%H:%M")

def get_current_datetime_str():
    """Get current datetime string for display"""
    current_dt = get_current_vietnam_datetime()
    return current_dt.strftime("%H:%M %d/%m/%Y")

print("üöÄ Flask News App:")
print(f"Gemini: {'‚úÖ' if GEMINI_API_KEY else '‚ùå'}")
print("=" * 20)

# üîß FREE RSS FEEDS ONLY - Removed ALL Paywall Sources 2025
RSS_FEEDS = {
    # === KINH T·∫æ TRONG N∆Ø·ªöC - CH·ªà CAFEF ===
    'domestic': {
        'cafef_chungkhoan': 'https://cafef.vn/thi-truong-chung-khoan.rss',
        'cafef_batdongsan': 'https://cafef.vn/bat-dong-san.rss',
        'cafef_taichinh': 'https://cafef.vn/tai-chinh-ngan-hang.rss',
        'cafef_vimo': 'https://cafef.vn/vi-mo-dau-tu.rss',
        'cafef_doanhnghiep': 'https://cafef.vn/doanh-nghiep.rss'
    },
    
    # === QU·ªêC T·∫æ - ONLY FREE RSS SOURCES (NO PAYWALL) ===
    'international': {
        # ‚úÖ YAHOO FINANCE RSS (100% Free)
        'yahoo_finance_main': 'https://finance.yahoo.com/news/rssindex',
        'yahoo_finance_headlines': 'https://feeds.finance.yahoo.com/rss/2.0/headline',
        'yahoo_finance_rss': 'https://www.yahoo.com/news/rss/finance',
        
        # ‚úÖ FREE NEWS RSS FEEDS (NO PAYWALL - Verified 2025)
        'cnn_money': 'http://rss.cnn.com/rss/money_topstories.rss',
        'marketwatch_latest': 'https://feeds.content.dowjones.io/public/rss/mw_topstories',
        'business_insider': 'http://feeds2.feedburner.com/businessinsider',
        'cnbc': 'https://www.cnbc.com/id/100003114/device/rss/rss.html',
        'investing_com': 'https://www.investing.com/rss/news.rss',
        'reuters_business': 'https://feeds.reuters.com/reuters/businessNews',
        'bbc_business': 'http://feeds.bbci.co.uk/news/business/rss.xml',
        'guardian_business': 'https://www.theguardian.com/business/economics/rss',
        'coindesk': 'https://feeds.feedburner.com/CoinDesk',
        'nasdaq_news': 'http://articlefeeds.nasdaq.com/nasdaq/categories?category=Investing+Ideas',
        
        # ‚úÖ FREE ALTERNATIVE SOURCES (Working 2025)
        'seeking_alpha': 'https://seekingalpha.com/feed.xml',
        'benzinga': 'https://www.benzinga.com/feed',
        'motley_fool': 'https://www.fool.com/feeds/index.aspx?format=rsslink&culture=en-us',
    }
}

def convert_utc_to_vietnam_time(utc_time_tuple):
    """Convert UTC to Vietnam time"""
    try:
        utc_timestamp = calendar.timegm(utc_time_tuple)
        utc_dt = datetime.fromtimestamp(utc_timestamp, tz=UTC_TIMEZONE)
        vn_dt = utc_dt.astimezone(VN_TIMEZONE)
        return vn_dt
    except Exception as e:
        return datetime.now(VN_TIMEZONE)

def normalize_title(title):
    """Normalize title for exact comparison"""
    import re
    # Convert to lowercase and remove extra spaces
    normalized = re.sub(r'\s+', ' ', title.lower().strip())
    # Remove common punctuation that might vary
    normalized = re.sub(r'[.,!?;:\-\u2013\u2014]', '', normalized)
    # Remove quotes that might vary
    normalized = re.sub(r'["\'\u201c\u201d\u2018\u2019]', '', normalized)
    return normalized

def clean_expired_cache():
    """Clean expired articles from global cache"""
    global global_seen_articles
    current_time = get_current_vietnam_datetime()
    expired_hashes = []
    
    for article_hash, article_data in global_seen_articles.items():
        time_diff = current_time - article_data['timestamp']
        if time_diff.total_seconds() > (CACHE_EXPIRE_HOURS * 3600):
            expired_hashes.append(article_hash)
    
    for expired_hash in expired_hashes:
        del global_seen_articles[expired_hash]
    
    if expired_hashes:
        print(f"üßπ Cleaned {len(expired_hashes)} expired articles from cache")

def is_duplicate_article_local(news_item, existing_articles):
    """Check duplicate within current collection - EXACT TITLE MATCH ONLY"""
    current_title = normalize_title(news_item['title'])
    current_link = news_item['link'].lower().strip()
    
    for existing in existing_articles:
        existing_title = normalize_title(existing['title'])
        existing_link = existing['link'].lower().strip()
        
        # Check exact title match OR exact link match
        if current_title == existing_title or current_link == existing_link:
            return True
    
    return False

def is_duplicate_article_global(news_item, source_name):
    """Check duplicate against global cache - EXACT TITLE MATCH ONLY"""
    global global_seen_articles
    
    try:
        # Clean expired cache first
        clean_expired_cache()
        
        current_title = normalize_title(news_item['title'])
        current_link = news_item['link'].lower().strip()
        
        # Check against global cache - EXACT matches only
        for existing_data in global_seen_articles.values():
            existing_title = normalize_title(existing_data['title'])
            existing_link = existing_data['link'].lower().strip()
            
            if current_title == existing_title or current_link == existing_link:
                return True
        
        # Add to global cache using simple key
        cache_key = f"{current_title}|{current_link}"
        
        global_seen_articles[cache_key] = {
            'title': news_item['title'],
            'link': news_item['link'],
            'source': source_name,
            'timestamp': get_current_vietnam_datetime()
        }
        
        # Limit cache size
        if len(global_seen_articles) > MAX_GLOBAL_CACHE:
            sorted_items = sorted(global_seen_articles.items(), key=lambda x: x[1]['timestamp'])
            for old_key, _ in sorted_items[:100]:
                del global_seen_articles[old_key]
        
        return False
        
    except Exception as e:
        print(f"‚ö†Ô∏è Global duplicate check error: {e}")
        return False

# üöÄ ASYNC HTTP CLIENT - NO MORE BLOCKING REQUESTS
async def fetch_with_aiohttp(url, headers=None, timeout=8):
    """FIXED: Use aiohttp instead of requests to prevent blocking"""
    try:
        if headers is None:
            headers = get_enhanced_headers(url)
        
        timeout_config = aiohttp.ClientTimeout(total=timeout)
        
        async with aiohttp.ClientSession(timeout=timeout_config, headers=headers) as session:
            async with session.get(url) as response:
                if response.status == 200:
                    content = await response.read()
                    return content
                else:
                    return None
    except Exception as e:
        print(f"‚ùå aiohttp fetch error for {url}: {e}")
        return None

def get_enhanced_headers(url=None):
    """Enhanced headers for better compatibility - NO BROTLI"""
    user_agent = random.choice(USER_AGENTS)
    
    headers = {
        'User-Agent': user_agent,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9,vi;q=0.8',
        'Accept-Encoding': 'gzip, deflate',  # Removed 'br' to avoid brotli errors
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'DNT': '1',
        'Cache-Control': 'no-cache',
        'Pragma': 'no-cache',
    }
    
    if url and 'yahoo' in url.lower():
        headers.update({
            'Referer': 'https://finance.yahoo.com/',
            'Origin': 'https://finance.yahoo.com',
        })
    elif url and 'cafef.vn' in url.lower():
        headers.update({
            'Referer': 'https://cafef.vn/',
            'Origin': 'https://cafef.vn'
        })
    
    return headers

def is_international_source(source_name):
    """Check if source is international - UPDATED for new sources"""
    international_sources = [
        'yahoo_finance', 'cnn_money', 'marketwatch_latest', 'business_insider',
        'cnbc', 'investing_com', 'reuters_business', 'bbc_business',
        'guardian_business', 'coindesk', 'nasdaq_news', 'seeking_alpha', 'benzinga', 'motley_fool'
    ]
    return any(source in source_name for source in international_sources)

def create_fallback_content(url, source_name, error_msg=""):
    """Create fallback content when extraction fails"""
    try:
        article_id = url.split('/')[-1] if '/' in url else 'news-article'
        
        if is_international_source(source_name):
            source_display = "Financial News"
            if 'marketwatch' in source_name:
                source_display = "MarketWatch"
            elif 'reuters' in source_name:
                source_display = "Reuters"
            elif 'cnn' in source_name:
                source_display = "CNN Money"
            elif 'cnbc' in source_name:
                source_display = "CNBC"
            elif 'bbc' in source_name:
                source_display = "BBC Business"
            elif 'motley_fool' in source_name:
                source_display = "Motley Fool"
            
            return f"""**{source_display} Financial News:**

üìà **Market Analysis:** This article provides financial market insights and economic analysis.

üìä **Coverage Areas:**
‚Ä¢ Real-time market data and analysis
‚Ä¢ Economic indicators and trends
‚Ä¢ Corporate earnings and reports
‚Ä¢ Investment strategies and forecasts

**Article ID:** {article_id}
**Note:** Content extraction failed. Please visit the original link for complete article.

{f'**Technical Error:** {error_msg}' if error_msg else ''}"""
        else:
            # Enhanced fallback for CafeF with more details
            return f"""**Tin t·ª©c kinh t·∫ø CafeF:**

üì∞ **Th√¥ng tin kinh t·∫ø:** B√†i vi·∫øt cung c·∫•p th√¥ng tin kinh t·∫ø, t√†i ch√≠nh t·ª´ CafeF.

üìä **N·ªôi dung chuy√™n s√¢u:**
‚Ä¢ Ph√¢n t√≠ch th·ªã tr∆∞·ªùng ch·ª©ng kho√°n Vi·ªát Nam  
‚Ä¢ Tin t·ª©c kinh t·∫ø vƒ© m√¥ v√† ch√≠nh s√°ch
‚Ä¢ B√°o c√°o doanh nghi·ªáp v√† t√†i ch√≠nh
‚Ä¢ B·∫•t ƒë·ªông s·∫£n v√† ƒë·∫ßu t∆∞

**üîç Chi ti·∫øt b√†i vi·∫øt:**
ƒê√¢y l√† b√†i vi·∫øt t·ª´ {source_name.replace('cafef_', 'CafeF ')} v·ªõi nhi·ªÅu th√¥ng tin h·ªØu √≠ch v·ªÅ th·ªã tr∆∞·ªùng t√†i ch√≠nh Vi·ªát Nam. 

**üí° N·ªôi dung bao g·ªìm:**
- Ph√¢n t√≠ch chuy√™n s√¢u t·ª´ c√°c chuy√™n gia
- S·ªë li·ªáu v√† bi·ªÉu ƒë·ªì c·∫≠p nh·∫≠t
- D·ª± b√°o xu h∆∞·ªõng th·ªã tr∆∞·ªùng
- Khuy·∫øn ngh·ªã ƒë·∫ßu t∆∞

**üì± L∆∞u √Ω:** ƒê·ªÉ ƒë·ªçc ƒë·∫ßy ƒë·ªß b√†i vi·∫øt v·ªõi h√¨nh ·∫£nh v√† bi·ªÉu ƒë·ªì, vui l√≤ng truy c·∫≠p link g·ªëc b√™n d∆∞·ªõi.

**M√£ b√†i vi·∫øt:** {article_id}

{f'**Th√¥ng tin k·ªπ thu·∫≠t:** {error_msg}' if error_msg else ''}"""
        
    except Exception as e:
        return f"N·ªôi dung t·ª´ {source_name}. Vui l√≤ng truy c·∫≠p link g·ªëc ƒë·ªÉ ƒë·ªçc ƒë·∫ßy ƒë·ªß."y ƒë·ªß, vui l√≤ng truy c·∫≠p link g·ªëc.

{f'**L·ªói:** {error_msg}' if error_msg else ''}"""
        
    except Exception as e:
        return f"N·ªôi dung t·ª´ {source_name}. Vui l√≤ng truy c·∫≠p link g·ªëc ƒë·ªÉ ƒë·ªçc ƒë·∫ßy ƒë·ªß."

async def extract_content_with_gemini(url, source_name):
    """Use Gemini to extract and translate content from international news"""
    try:
        if not GEMINI_API_KEY or not GEMINI_AVAILABLE:
            return create_fallback_content(url, source_name, "Gemini kh√¥ng kh·∫£ d·ª•ng")
        
        extraction_prompt = f"""Truy c·∫≠p v√† tr√≠ch xu·∫•t TO√ÄN B·ªò n·ªôi dung b√†i b√°o t·ª´: {url}

Y√äU C·∫¶U:
1. ƒê·ªçc v√† hi·ªÉu HO√ÄN TO√ÄN b√†i b√°o
2. Tr√≠ch xu·∫•t T·∫§T C·∫¢ n·ªôi dung ch√≠nh (lo·∫°i b·ªè qu·∫£ng c√°o, sidebar)
3. D·ªãch t·ª´ ti·∫øng Anh sang ti·∫øng Vi·ªát T·ª∞ NHI√äN
4. Gi·ªØ nguy√™n s·ªë li·ªáu, t√™n c√¥ng ty, thu·∫≠t ng·ªØ t√†i ch√≠nh
5. ƒê·ªô d√†i: 500-1500 t·ª´ (to√†n b·ªô b√†i vi·∫øt)
6. CH·ªà tr·∫£ v·ªÅ n·ªôi dung b√†i b√°o ƒë√£ d·ªãch

**N·ªòI DUNG HO√ÄN CH·ªàNH:**"""

        try:
            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            generation_config = genai.types.GenerationConfig(
                temperature=0.1,
                top_p=0.8,
                max_output_tokens=3000,  # TƒÉng t·ª´ 2000 ƒë·ªÉ l·∫•y to√†n b·ªô n·ªôi dung
            )
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    model.generate_content,
                    extraction_prompt,
                    generation_config=generation_config
                ),
                timeout=30  # TƒÉng timeout t·ª´ 20s
            )
            
            extracted_content = response.text.strip()
            
            if len(extracted_content) > 300:
                error_indicators = [
                    'cannot access', 'unable to access', 'kh√¥ng th·ªÉ truy c·∫≠p',
                    'failed to retrieve', 'error occurred', 'sorry, i cannot'
                ]
                
                if not any(indicator in extracted_content.lower() for indicator in error_indicators):
                    return f"[ü§ñ Gemini AI - To√†n b·ªô n·ªôi dung t·ª´ {source_name}]\n\n{extracted_content}"
                else:
                    return create_fallback_content(url, source_name, "Gemini kh√¥ng th·ªÉ tr√≠ch xu·∫•t")
            else:
                return create_fallback_content(url, source_name, "N·ªôi dung qu√° ng·∫Øn")
            
        except asyncio.TimeoutError:
            return create_fallback_content(url, source_name, "Gemini timeout")
        except Exception as e:
            return create_fallback_content(url, source_name, f"L·ªói Gemini: {str(e)}")
            
    except Exception as e:
        return create_fallback_content(url, source_name, str(e))

# üöÄ ASYNC-FIRST APPROACHES - NO MORE BLOCKING
async def async_sleep_delay():
    """FIXED: Use asyncio.sleep instead of time.sleep to prevent heartbeat blocking"""
    delay = random.uniform(0.1, 0.5)  # Much shorter delay
    await asyncio.sleep(delay)

def clean_content_enhanced(content):
    """Enhanced content cleaning for CafeF"""
    if not content:
        return content
    
    unwanted_patterns = [
        r'Theo.*?CafeF.*?',
        r'Ngu·ªìn.*?:.*?',
        r'Tags:.*?$',
        r'T·ª´ kh√≥a:.*?$',
        r'ƒêƒÉng k√Ω.*?nh·∫≠n tin.*?',
        r'Like.*?Fanpage.*?',
        r'Follow.*?us.*?'
    ]
    
    for pattern in unwanted_patterns:
        content = re.sub(pattern, '', content, flags=re.IGNORECASE | re.DOTALL)
    
    content = re.sub(r'\s+', ' ', content)
    content = re.sub(r'\n\s*\n', '\n', content)
    
    return content.strip()

# üöÄ ASYNC CONTENT EXTRACTION - Non-blocking
async def extract_content_enhanced(url, source_name, news_item=None):
    """Enhanced content extraction - Gemini for international, traditional for domestic"""
    
    # For international sources, use Gemini
    if is_international_source(source_name):
        print(f"ü§ñ Using Gemini for international source: {source_name}")
        return await extract_content_with_gemini(url, source_name)
    
    # For domestic (CafeF) sources, use traditional async methods
    try:
        print(f"üîß Using async traditional methods for domestic source: {source_name}")
        await async_sleep_delay()
        
        content = await fetch_with_aiohttp(url)
        
        if content:
            # Method 1: Trafilatura with enhanced config for full content
            if TRAFILATURA_AVAILABLE:
                try:
                    result = await asyncio.to_thread(
                        trafilatura.bare_extraction,
                        content,
                        include_comments=False,
                        include_tables=True,
                        include_links=False,
                        include_images=False,
                        favor_precision=False,  # Changed to False for more content
                        favor_recall=True,      # Added for maximum content
                        with_metadata=True,
                        prune_xpath=[],         # Don't prune anything
                        only_with_metadata=False
                    )
                    
                    if result and result.get('text') and len(result['text']) > 200:
                        full_text = result['text']
                        
                        # Try to get more content with different settings
                        if len(full_text) < 1000:
                            result2 = await asyncio.to_thread(
                                trafilatura.extract,
                                content,
                                include_comments=True,
                                include_tables=True,
                                include_links=True,
                                favor_precision=False,
                                favor_recall=True
                            )
                            if result2 and len(result2) > len(full_text):
                                full_text = result2
                        
                        return full_text.strip()
                except Exception as e:
                    print(f"‚ö†Ô∏è Trafilatura failed: {e}")
            
            # Method 2: Enhanced BeautifulSoup with multiple strategies
            if BEAUTIFULSOUP_AVAILABLE:
                try:
                    soup = await asyncio.to_thread(BeautifulSoup, content, 'html.parser')
                    
                    # Strategy 1: CafeF specific selectors
                    content_selectors = [
                        'div.detail-content',
                        'div.fck_detail', 
                        'div.content-detail',
                        'div.article-content',
                        'div.entry-content',
                        'div.post-content',
                        'article',
                        'main',
                        '.article-body',
                        '.content-body',
                        '.post-body'
                    ]
                    
                    extracted_text = ""
                    for selector in content_selectors:
                        elements = soup.select(selector)
                        if elements:
                            for element in elements:
                                text = element.get_text(strip=True)
                                if len(text) > len(extracted_text):
                                    extracted_text = text
                    
                    # Strategy 2: Find all paragraphs and combine
                    if len(extracted_text) < 500:
                        all_paragraphs = soup.find_all('p')
                        paragraph_texts = []
                        for p in all_paragraphs:
                            p_text = p.get_text(strip=True)
                            if len(p_text) > 50:  # Only substantial paragraphs
                                paragraph_texts.append(p_text)
                        
                        combined_text = '\n\n'.join(paragraph_texts)
                        if len(combined_text) > len(extracted_text):
                            extracted_text = combined_text
                    
                    if extracted_text and len(extracted_text) > 300:
                        cleaned_content = clean_content_enhanced(extracted_text)
                        return cleaned_content.strip()
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è BeautifulSoup failed: {e}")
            
            # Method 3: Newspaper3k fallback
            if NEWSPAPER_AVAILABLE:
                try:
                    from newspaper import Article
                    article = Article(url)
                    article.set_config({
                        'headers': get_enhanced_headers(url),
                        'timeout': 12
                    })
                    
                    article.download()
                    article.parse()
                    
                    if article.text and len(article.text) > 300:
                        return article.text.strip()
                
                except Exception as e:
                    print(f"‚ö†Ô∏è Newspaper3k failed: {e}")
        
        print(f"‚ö†Ô∏è All traditional methods failed for {source_name}")
        return create_fallback_content(url, source_name, "Traditional extraction methods failed")
        
    except Exception as e:
        print(f"‚ùå Extract content error for {source_name}: {e}")
        return create_fallback_content(url, source_name, str(e))

async def process_rss_feed_async(source_name, rss_url, limit_per_source):
    """FIXED: Async RSS feed processing to prevent blocking"""
    try:
        await async_sleep_delay()
        
        # Use aiohttp instead of requests
        content = await fetch_with_aiohttp(rss_url)
        
        if content:
            # Parse feedparser in thread to avoid blocking
            feed = await asyncio.to_thread(feedparser.parse, content)
        else:
            # Fallback to direct feedparser
            feed = await asyncio.to_thread(feedparser.parse, rss_url)
        
        if not feed or not hasattr(feed, 'entries') or len(feed.entries) == 0:
            return []
        
        news_items = []
        for entry in feed.entries[:limit_per_source]:
            try:
                vn_time = get_current_vietnam_datetime()
                
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    vn_time = convert_utc_to_vietnam_time(entry.published_parsed)
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    vn_time = convert_utc_to_vietnam_time(entry.updated_parsed)
                
                description = ""
                if hasattr(entry, 'summary'):
                    description = entry.summary[:400] + "..." if len(entry.summary) > 400 else entry.summary
                elif hasattr(entry, 'description'):
                    description = entry.description[:400] + "..." if len(entry.description) > 400 else entry.description
                
                if hasattr(entry, 'title') and hasattr(entry, 'link'):
                    title = entry.title.strip()
                    
                    # Filter for relevant economic/financial content
                    if is_relevant_news(title, description, source_name):
                        news_item = {
                            'title': html.unescape(title),
                            'link': entry.link,
                            'source': source_name,
                            'published': vn_time,
                            'published_str': vn_time.strftime("%H:%M %d/%m"),
                            'description': html.unescape(description) if description else ""
                        }
                        news_items.append(news_item)
                
            except Exception as entry_error:
                continue
        
        print(f"‚úÖ Processed {len(news_items)} articles from {source_name}")
        return news_items
        
    except Exception as e:
        print(f"‚ùå RSS processing error for {source_name}: {e}")
        return []

def is_relevant_news(title, description, source_name):
    """Filter for relevant economic/financial news - MORE RELAXED"""
    # For CafeF sources, all content is relevant
    if 'cafef' in source_name:
        return True
    
    # For international sources, use very relaxed filter
    financial_keywords = [
        'stock', 'market', 'trading', 'investment', 'economy', 'economic',
        'bitcoin', 'crypto', 'currency', 'bank', 'financial', 'finance',
        'earnings', 'revenue', 'profit', 'inflation', 'fed', 'gdp',
        'business', 'company', 'corporate', 'industry', 'sector',
        'money', 'cash', 'capital', 'fund', 'price', 'cost', 'value',
        'growth', 'analyst', 'forecast', 'report', 'data', 'sales'
    ]
    
    title_lower = title.lower()
    description_lower = description.lower() if description else ""
    
    # Check in both title and description
    for keyword in financial_keywords:
        if keyword in title_lower or keyword in description_lower:
            return True
    
    # If no keywords found, still include (very relaxed)
    return True  # Changed from False to True - accept all articles

async def process_single_source(source_name, source_url, limit_per_source):
    """Process a single RSS source asynchronously"""
    try:
        print(f"üîÑ Processing {source_name}: {source_url}")
        
        if source_url.endswith('.rss') or 'rss' in source_url.lower() or 'feeds.' in source_url:
            # RSS Feed processing
            return await process_rss_feed_async(source_name, source_url, limit_per_source)
        else:
            # For future expansion - direct scraping
            return []
            
    except Exception as e:
        print(f"‚ùå Error for {source_name}: {e}")
        return []

# üöÄ ASYNC NEWS COLLECTION - Fully non-blocking
async def collect_news_enhanced(sources_dict, limit_per_source=15, use_global_dedup=False):
    """Session-based collection with EXACT TITLE duplicate detection"""
    all_news = []
    
    print(f"üîÑ Starting collection from {len(sources_dict)} sources (Global dedup: {use_global_dedup})")
    print(f"üéØ Duplicate logic: EXACT title match only")
    
    # Clean expired cache before starting
    if use_global_dedup:
        clean_expired_cache()
    
    # Create tasks for concurrent processing
    tasks = []
    for source_name, source_url in sources_dict.items():
        task = process_single_source(source_name, source_url, limit_per_source)
        tasks.append(task)
    
    # Process all sources concurrently
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Collect results with exact title duplicate detection
    total_processed = 0
    local_duplicates = 0
    global_duplicates = 0
    
    for result in results:
        if isinstance(result, Exception):
            print(f"‚ùå Source processing error: {result}")
        elif result:
            for news_item in result:
                total_processed += 1
                
                # Local duplicate check (exact title/link match within current collection)
                if is_duplicate_article_local(news_item, all_news):
                    local_duplicates += 1
                    print(f"üîÑ Local duplicate: {news_item['title'][:50]}...")
                    continue
                
                # Global duplicate check (exact title/link match cross-session) - only if enabled
                if use_global_dedup and is_duplicate_article_global(news_item, news_item['source']):
                    global_duplicates += 1
                    print(f"üåç Global duplicate: {news_item['title'][:50]}...")
                    continue
                
                # Add unique article
                all_news.append(news_item)
    
    unique_count = len(all_news)
    print(f"üìä Processed: {total_processed}, Local dups: {local_duplicates}, Global dups: {global_duplicates}, Unique: {unique_count}")
    
    # Sort by publish time
    all_news.sort(key=lambda x: x['published'], reverse=True)
    return all_news

def get_or_create_user_session():
    """Get or create user session ID"""
    if 'user_id' not in session:
        session['user_id'] = str(uuid.uuid4())
    return session['user_id']

def save_user_news_enhanced(user_id, news_list, command_type, current_page=1):
    """Enhanced user news saving with pagination info"""
    global user_news_cache
    
    user_news_cache[user_id] = {
        'news': news_list,  # Full news list
        'command': command_type,
        'current_page': current_page,
        'timestamp': get_current_vietnam_datetime()
    }
    
    if len(user_news_cache) > MAX_CACHE_ENTRIES:
        oldest_users = sorted(user_news_cache.items(), key=lambda x: x[1]['timestamp'])[:10]
        for user_id_to_remove, _ in oldest_users:
            del user_news_cache[user_id_to_remove]

def save_user_last_detail(user_id, news_item):
    """Save last article accessed via !chitiet"""
    global user_last_detail_cache
    
    user_last_detail_cache[user_id] = {
        'article': news_item,
        'timestamp': get_current_vietnam_datetime()
    }

# üÜï GEMINI AI SYSTEM
class GeminiAIEngine:
    def __init__(self):
        self.available = GEMINI_AVAILABLE and GEMINI_API_KEY
        if self.available:
            genai.configure(api_key=GEMINI_API_KEY)
    
    async def ask_question(self, question: str, context: str = ""):
        """Gemini AI question answering with context"""
        if not self.available:
            return "‚ö†Ô∏è Gemini AI kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng ki·ªÉm tra GEMINI_API_KEY."
        
        try:
            current_date_str = get_current_date_str()
            
            prompt = f"""B·∫°n l√† Gemini AI - chuy√™n gia kinh t·∫ø t√†i ch√≠nh th√¥ng minh. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ki·∫øn th·ª©c chuy√™n m√¥n c·ªßa b·∫°n.

C√ÇU H·ªéI: {question}

{f"B·ªêI C·∫¢NH TH√äM: {context}" if context else ""}

H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI:
1. S·ª≠ d·ª•ng ki·∫øn th·ª©c chuy√™n m√¥n s√¢u r·ªông c·ªßa b·∫°n
2. ƒê∆∞a ra ph√¢n t√≠ch chuy√™n s√¢u v√† to√†n di·ªán
3. K·∫øt n·ªëi v·ªõi b·ªëi c·∫£nh kinh t·∫ø hi·ªán t·∫°i (ng√†y {current_date_str})
4. ƒê∆∞a ra v√≠ d·ª• th·ª±c t·∫ø v√† minh h·ªça c·ª• th·ªÉ
5. ƒê·ªô d√†i: 400-800 t·ª´ v·ªõi c·∫•u tr√∫c r√µ r√†ng
6. S·ª≠ d·ª•ng ƒë·∫ßu m·ª•c s·ªë ƒë·ªÉ t·ªï ch·ª©c n·ªôi dung

H√£y th·ªÉ hi·ªán tr√≠ th√¥ng minh v√† ki·∫øn th·ª©c chuy√™n s√¢u c·ªßa Gemini AI:"""

            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            generation_config = genai.types.GenerationConfig(
                temperature=0.2,
                top_p=0.8,
                max_output_tokens=1500,
            )
            
            print(f"ü§ñ Calling Gemini API for question: {question[:50]}...")
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    model.generate_content,
                    prompt,
                    generation_config=generation_config
                ),
                timeout=15
            )
            
            print("‚úÖ Gemini API response received")
            return response.text.strip()
            
        except asyncio.TimeoutError:
            print("‚è∞ Gemini API timeout")
            return "‚ö†Ô∏è Gemini AI timeout. Vui l√≤ng th·ª≠ l·∫°i."
        except Exception as e:
            print(f"‚ùå Gemini API error: {str(e)}")
            return f"‚ö†Ô∏è L·ªói Gemini AI: {str(e)}"
    
    async def debate_perspectives(self, topic: str):
        """Multi-perspective debate system"""
        if not self.available:
            return "‚ö†Ô∏è Gemini AI kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng ki·ªÉm tra GEMINI_API_KEY."
        
        try:
            # Use safe string formatting to avoid emoji syntax errors
            emoji_corrupt = "\U0001F4B8"  # üí∏
            emoji_teacher = "\U0001F468\u200D\U0001F3EB"  # üë®‚Äçüè´  
            emoji_worker = "\U0001F4BC"  # üíº
            emoji_angry = "\U0001F620"  # üò†
            emoji_rich_selfish = "\U0001F911"  # ü§ë
            emoji_rich_wise = "\U0001F9E0"  # üß†
            emoji_robot = "\U0001F916"  # ü§ñ
            
            prompt = f"""T·ªï ch·ª©c cu·ªôc tranh lu·∫≠n v·ªÅ: {topic}

6 quan ƒëi·ªÉm kh√°c nhau:
{emoji_corrupt} **Nh√† KT Tham Nh≈©ng:** [√≠ch k·ª∑, b√≥p m√©o s·ªë li·ªáu]
{emoji_teacher} **GS Ch√≠nh Tr·ª±c:** [h·ªçc thu·∫≠t, ƒë·∫°o ƒë·ª©c cao]  
{emoji_worker} **Nh√¢n Vi√™n Ham Ti·ªÅn:** [ch·ªâ quan t√¢m l∆∞∆°ng]
{emoji_angry} **Ng∆∞·ªùi Ngh√®o:** [ƒë·ªï l·ªói, thi·∫øu hi·ªÉu bi·∫øt]
{emoji_rich_selfish} **Ng∆∞·ªùi Gi√†u √çch K·ª∑:** [ch·ªâ t√¨m l·ª£i nhu·∫≠n]
{emoji_rich_wise} **Ng∆∞·ªùi Gi√†u Th√¥ng Th√°i:** [t·∫ßm nh√¨n xa]
{emoji_robot} **T·ªïng K·∫øt:** [ph√¢n t√≠ch kh√°ch quan]

M·ªói g√≥c nh√¨n 80-120 t·ª´, th·ªÉ hi·ªán r√µ t√≠nh c√°ch:"""

            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            generation_config = genai.types.GenerationConfig(
                temperature=0.4,
                top_p=0.9,
                max_output_tokens=1500,
            )
            
            print(f"üé≠ Calling Gemini API for debate: {topic[:50]}...")
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    model.generate_content,
                    prompt,
                    generation_config=generation_config
                ),
                timeout=20
            )
            
            print("‚úÖ Gemini API debate response received")
            return response.text.strip()
            
        except asyncio.TimeoutError:
            print("‚è∞ Gemini API debate timeout")
            return "‚ö†Ô∏è Gemini AI timeout."
        except Exception as e:
            print(f"‚ùå Gemini API debate error: {str(e)}")
            return f"‚ö†Ô∏è L·ªói Gemini AI: {str(e)}"
    
    async def analyze_article(self, article_content: str, question: str = ""):
        """Analyze specific article with Gemini - Vietnamese response"""
        if not self.available:
            return "Gemini AI kh√¥ng kh·∫£ d·ª•ng cho ph√¢n t√≠ch b√†i b√°o."
        
        try:
            analysis_question = question if question else "H√£y ph√¢n t√≠ch v√† t√≥m t·∫Øt b√†i b√°o n√†y"
            
            prompt = f"""B·∫°n l√† Gemini AI - chuy√™n gia kinh t·∫ø t√†i ch√≠nh th√¥ng minh. H√£y ph√¢n t√≠ch b√†i b√°o d·ª±a tr√™n n·ªôi dung ho√†n ch·ªânh ƒë∆∞·ª£c cung c·∫•p.

N·ªòI DUNG B√ÄI B√ÅO HO√ÄN CH·ªàNH:
{article_content}

Y√äU C·∫¶U PH√ÇN T√çCH:
{analysis_question}

H∆Ø·ªöNG D·∫™N PH√ÇN T√çCH:
1. Ph√¢n t√≠ch ch·ªß y·∫øu d·ª±a tr√™n n·ªôi dung b√†i b√°o (85-90%)
2. K·∫øt h·ª£p ki·∫øn th·ª©c chuy√™n m√¥n ƒë·ªÉ gi·∫£i th√≠ch s√¢u h∆°n (10-15%)
3. Ph√¢n t√≠ch t√°c ƒë·ªông, nguy√™n nh√¢n, h·∫≠u qu·∫£
4. ƒê∆∞a ra nh·∫≠n ƒë·ªãnh v√† ƒë√°nh gi√° chuy√™n s√¢u
5. Tr·∫£ l·ªùi c√¢u h·ªèi tr·ª±c ti·∫øp v·ªõi b·∫±ng ch·ª©ng t·ª´ b√†i b√°o
6. ƒê·ªô d√†i: 600-1000 t·ª´ v·ªõi c·∫•u tr√∫c r√µ r√†ng
7. Tham chi·∫øu c√°c ph·∫ßn c·ª• th·ªÉ trong b√†i b√°o
8. CH·ªà ph√¢n t√≠ch b√†i b√°o ƒë∆∞·ª£c cung c·∫•p

T·∫≠p trung ho√†n to√†n v√†o n·ªôi dung t·ª´ b√†i b√°o ƒë√£ cung c·∫•p. ƒê∆∞a ra ph√¢n t√≠ch th√¥ng minh v√† chi ti·∫øt b·∫±ng ti·∫øng Vi·ªát:"""

            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            
            generation_config = genai.types.GenerationConfig(
                temperature=0.2,
                top_p=0.8,
                max_output_tokens=2000,
            )
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    model.generate_content,
                    prompt,
                    generation_config=generation_config
                ),
                timeout=20
            )
            
            return response.text.strip()
            
        except asyncio.TimeoutError:
            return "Gemini AI timeout khi ph√¢n t√≠ch b√†i b√°o."
        except Exception as e:
            return f"L·ªói Gemini AI: {str(e)}"

# Initialize Gemini Engine
gemini_engine = GeminiAIEngine()

# Source names mapping for display
source_names = {
    # CafeF sources
    'cafef_chungkhoan': 'CafeF CK', 'cafef_batdongsan': 'CafeF BƒêS',
    'cafef_taichinh': 'CafeF TC', 'cafef_vimo': 'CafeF VM', 'cafef_doanhnghiep': 'CafeF DN',
    
    # FREE international sources - UPDATED
    'yahoo_finance_main': 'Yahoo RSS', 'yahoo_finance_headlines': 'Yahoo Headlines',
    'yahoo_finance_rss': 'Yahoo Finance', 'cnn_money': 'CNN Money', 
    'marketwatch_latest': 'MarketWatch', 'business_insider': 'Business Insider',
    'cnbc': 'CNBC', 'investing_com': 'Investing.com', 
    'reuters_business': 'Reuters Business', 'bbc_business': 'BBC Business', 
    'guardian_business': 'The Guardian', 'coindesk': 'CoinDesk', 
    'nasdaq_news': 'Nasdaq', 'seeking_alpha': 'Seeking Alpha', 'benzinga': 'Benzinga',
    'motley_fool': 'Motley Fool'
}

emoji_map = {
    # CafeF sources
    'cafef_chungkhoan': 'üìà', 'cafef_batdongsan': 'üè¢', 'cafef_taichinh': 'üí∞', 
    'cafef_vimo': 'üìä', 'cafef_doanhnghiep': 'üè≠',
    
    # FREE international sources - UPDATED
    'yahoo_finance_main': 'üíº', 'yahoo_finance_headlines': 'üì∞', 'yahoo_finance_rss': 'üíº',
    'cnn_money': 'üì∫', 'marketwatch_latest': 'üìä', 'business_insider': 'üíº', 
    'cnbc': 'üì∫', 'investing_com': 'üíπ', 'reuters_business': 'üåç',
    'bbc_business': 'üá¨üáß', 'guardian_business': 'üõ°Ô∏è', 'coindesk': '‚Çø',
    'nasdaq_news': 'üìà', 'seeking_alpha': 'üîç', 'benzinga': 'üöÄ', 'motley_fool': 'ü§°'
}

# Flask Routes
@app.route('/')
def index():
    """Main page"""
    return render_template('index.html')

@app.route('/api/news/<news_type>')
async def get_news_api(news_type):
    """API endpoint for getting news"""
    try:
        page = int(request.args.get('page', 1))
        user_id = get_or_create_user_session()
        
        if news_type == 'all':
            # Concurrent collection
            domestic_task = collect_news_enhanced(RSS_FEEDS['domestic'], 15)
            international_task = collect_news_enhanced(RSS_FEEDS['international'], 20)
            
            domestic_news, international_news = await asyncio.gather(domestic_task, international_task)
            all_news = domestic_news + international_news
            
        elif news_type == 'domestic':
            all_news = await collect_news_enhanced(RSS_FEEDS['domestic'], 15)
            
        elif news_type == 'international':
            all_news = await collect_news_enhanced(RSS_FEEDS['international'], 20)
            
        else:
            return jsonify({'error': 'Invalid news type'}), 400
        
        items_per_page = 12
        start_index = (page - 1) * items_per_page
        end_index = start_index + items_per_page
        page_news = all_news[start_index:end_index]
        
        # Save to user cache
        save_user_news_enhanced(user_id, all_news, f"{news_type}_page_{page}")
        
        # Format news for frontend
        formatted_news = []
        for i, news in enumerate(page_news):
            emoji = emoji_map.get(news['source'], 'üì∞')
            source_display = source_names.get(news['source'], news['source'])
            
            formatted_news.append({
                'id': start_index + i,
                'title': news['title'],
                'link': news['link'],
                'source': source_display,
                'emoji': emoji,
                'published': news['published_str'],
                'description': news['description'][:200] + "..." if len(news['description']) > 200 else news['description']
            })
        
        total_pages = (len(all_news) + items_per_page - 1) // items_per_page
        
        return jsonify({
            'news': formatted_news,
            'page': page,
            'total_pages': total_pages,
            'total_articles': len(all_news)
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/article/<int:article_id>')
async def get_article_detail(article_id):
    """Get article detail"""
    try:
        user_id = get_or_create_user_session()
        
        if user_id not in user_news_cache:
            return jsonify({'error': 'No cached news found'}), 404
            
        user_data = user_news_cache[user_id]
        news_list = user_data['news']
        
        if article_id < 0 or article_id >= len(news_list):
            return jsonify({'error': 'Invalid article ID'}), 404
            
        news = news_list[article_id]
        
        # Save as last detail for AI context
        save_user_last_detail(user_id, news)
        
        # Extract content
        full_content = await extract_content_enhanced(news['link'], news['source'], news)
        
        source_display = source_names.get(news['source'], news['source'])
        
        return jsonify({
            'title': news['title'],
            'content': full_content,
            'source': source_display,
            'published': news['published_str'],
            'link': news['link']
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/ai/ask', methods=['POST'])
async def ai_ask():
    """AI ask endpoint"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400
            
        question = data.get('question', '')
        user_id = get_or_create_user_session()
        
        print(f"ü§ñ AI Ask - User: {user_id}, Question: '{question}'")
        
        # Check for recent article context
        context = ""
        if user_id in user_last_detail_cache:
            last_detail = user_last_detail_cache[user_id]
            time_diff = get_current_vietnam_datetime() - last_detail['timestamp']
            
            if time_diff.total_seconds() < 1800:  # 30 minutes
                article = last_detail['article']
                
                # Extract content for context
                article_content = await extract_content_enhanced(article['link'], article['source'], article)
                
                if article_content:
                    context = f"B√ÄI B√ÅO LI√äN QUAN:\nTi√™u ƒë·ªÅ: {article['title']}\nNgu·ªìn: {article['source']}\nN·ªôi dung: {article_content[:1500]}"
                    print(f"üìÑ Found article context: {article['title'][:50]}...")
        
        # Get AI response
        if context and not question:
            # Auto-summarize if no question provided
            print("üîÑ Auto-summarizing article")
            response = await gemini_engine.analyze_article(context, "H√£y t√≥m t·∫Øt c√°c √Ω ch√≠nh c·ªßa b√†i b√°o n√†y")
        elif context:
            print("‚ùì Answering question with context")
            response = await gemini_engine.analyze_article(context, question)
        else:
            print("üí≠ General question without context")
            response = await gemini_engine.ask_question(question, context)
        
        return jsonify({'response': response})
        
    except Exception as e:
        print(f"‚ùå AI Ask Error: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/ai/debate', methods=['POST'])
async def ai_debate():
    """AI debate endpoint"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400
            
        topic = data.get('topic', '')
        user_id = get_or_create_user_session()
        
        print(f"üé≠ AI Debate - User: {user_id}, Topic: '{topic}'")
        
        # Check for context if no topic provided
        if not topic:
            if user_id in user_last_detail_cache:
                last_detail = user_last_detail_cache[user_id]
                time_diff = get_current_vietnam_datetime() - last_detail['timestamp']
                
                if time_diff.total_seconds() < 1800:
                    article = last_detail['article']
                    topic = f"B√†i b√°o: {article['title']}"
                    print(f"üìÑ Using article as debate topic: {topic[:50]}...")
                else:
                    print("‚è∞ No recent article context")
                    return jsonify({'error': 'Kh√¥ng c√≥ ch·ªß ƒë·ªÅ ƒë·ªÉ b√†n lu·∫≠n v√† kh√¥ng c√≥ b√†i b√°o g·∫ßn ƒë√¢y'}), 400
            else:
                print("‚ùå No topic and no article context")
                return jsonify({'error': 'C·∫ßn nh·∫≠p ch·ªß ƒë·ªÅ ƒë·ªÉ b√†n lu·∫≠n ho·∫∑c xem b√†i b√°o tr∆∞·ªõc'}), 400
        
        print(f"üöÄ Starting debate with topic: {topic}")
        response = await gemini_engine.debate_perspectives(topic)
        
        return jsonify({'response': response})
        
    except Exception as e:
        print(f"‚ùå AI Debate Error: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/test')
def test_route():
    """Simple test route"""
    return "Flask app is working!"

@app.route('/api/status')
def api_status():
    """Simple API status check"""
    return jsonify({
        'status': 'OK',
        'message': 'API is working',
        'gemini_available': gemini_engine.available if 'gemini_engine' in globals() else False,
        'timestamp': get_current_datetime_str()
    })

@app.route('/api/debug')
def debug_status():
    """Debug endpoint to check system status"""
    try:
        return jsonify({
            'gemini_available': gemini_engine.available,
            'gemini_api_key_set': bool(GEMINI_API_KEY),
            'gemini_api_key_preview': GEMINI_API_KEY[:10] + "..." if GEMINI_API_KEY else None,
            'current_time': get_current_datetime_str(),
            'total_sources': len(RSS_FEEDS['domestic']) + len(RSS_FEEDS['international']),
            'cache_size': len(global_seen_articles)
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 8080)), debug=False)
